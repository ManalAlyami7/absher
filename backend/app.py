"""
========================================
Tanabbah - Enhanced API Server v2.2
========================================
Complete analysis with technical details
Author: Manal Alyami
Version: 2.2.0 - Full Technical Insights
========================================
"""

import os
import logging
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict

from backend.ml import predict_url, extract_urls, MODEL_LOADED, URLPrediction
from backend.llm import analyze_message_with_llm, is_llm_available, LLMAnalysis, is_trusted_domain

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Tanabbah Enhanced API v2.2",
    description="AI-powered phishing detection with complete technical insights",
    version="2.2.0"
)

ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "*").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)


class AnalyzeRequest(BaseModel):
    message: str
    enable_llm: Optional[bool] = True
    language: Optional[str] = "ar"  # Interface language


class TechnicalDetails(BaseModel):
    urls_found: int
    url_types: List[str]
    ml_risk_score: float
    llm_confidence: Optional[float]
    trusted_source: bool
    red_flags_details: List[str]
    analysis_method: str
    features_analyzed: int


class AnalyzeResponse(BaseModel):
    message: str
    classification: str
    classification_ar: str
    explanation: str
    explanation_ar: str
    red_flags: List[str]
    red_flags_ar: List[str]
    risk_score: float
    action: str
    action_ar: str
    technical_details: TechnicalDetails
    urls_found: int
    url_predictions: List[URLPrediction]
    ml_risk_score: float
    llm_analysis: Optional[LLMAnalysis]
    combined_risk_score: float
    status: str


def classify_url_type(probability: float, url: str) -> str:
    """Classify URL based on probability and characteristics"""
    if is_trusted_domain(url):
        return "Trusted"
    elif probability >= 0.8:
        return "Phishing"
    elif probability >= 0.6:
        return "Suspicious"
    elif probability >= 0.4:
        return "Lookalike"
    else:
        return "Safe"


def get_action_guidance(classification: str, language: str = "ar") -> tuple:
    """Get action guidance based on classification"""
    actions = {
        "SAFE": {
            "ar": "âœ… Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ø·Ù„ÙˆØ¨",
            "en": "âœ… No action required"
        },
        "LOW_RISK": {
            "ar": "âš ï¸ ØªØ­Ù‚Ù‚ Ù‚Ø¨Ù„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø£ÙŠ Ø±Ø§Ø¨Ø·",
            "en": "âš ï¸ Verify before clicking any links"
        },
        "SUSPICIOUS": {
            "ar": "ğŸš« Ù„Ø§ ØªØ¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø­ØªÙ‰ ØªØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±",
            "en": "ğŸš« Do not click links until you verify the source"
        },
        "HIGH_RISK": {
            "ar": "âŒ ÙŠÙÙ†ØµØ­ Ø¨Ø­Ø°Ù Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙˆØ¹Ø¯Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹Ù‡Ø§",
            "en": "âŒ Recommended to delete and not interact"
        }
    }
    
    action_dict = actions.get(classification, actions["SAFE"])
    return action_dict.get(language, action_dict["ar"])


def get_explanation(classification: str, risk_score: float, 
                    has_trusted: bool, language: str = "ar") -> str:
    """Generate explanation based on classification"""
    
    if language == "ar":
        if classification == "SAFE":
            if has_trusted:
                return f"Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªØ¨Ø¯Ùˆ Ø±Ø³Ù…ÙŠØ© ÙˆØµØ§Ø¯Ø±Ø© Ù…Ù† Ø¬Ù‡Ø© Ù…ÙˆØ«ÙˆÙ‚Ø©. Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ø­ØªÙŠØ§Ù„ ÙˆØ§Ø¶Ø­Ø©. (Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·Ø±: {risk_score}%)"
            return f"Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªØ¨Ø¯Ùˆ Ø¢Ù…Ù†Ø© Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…. Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø®Ø·Ø± ÙˆØ§Ø¶Ø­Ø©. (Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·Ø±: {risk_score}%)"
        elif classification == "LOW_RISK":
            return f"Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ³ØªØ¯Ø¹ÙŠ Ø§Ù„Ø­Ø°Ø± Ø§Ù„Ù…Ø¹ØªØ¯Ù„. ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±. (Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·Ø±: {risk_score}%)"
        elif classification == "SUSPICIOUS":
            return f"Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©. ØªÙˆØ®ÙÙ‘ Ø§Ù„Ø­Ø°Ø± Ø§Ù„Ø´Ø¯ÙŠØ¯ ÙˆÙ„Ø§ ØªØ¶ØºØ· Ø¹Ù„Ù‰ Ø£ÙŠ Ø±ÙˆØ§Ø¨Ø·. (Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·Ø±: {risk_score}%)"
        else:  # HIGH_RISK
            return f"Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ø­ØªÙŠØ§Ù„ Ù‚ÙˆÙŠØ©. Ù„Ø§ ØªØªÙØ§Ø¹Ù„ Ù…Ø¹Ù‡Ø§ ÙˆÙŠÙÙ†ØµØ­ Ø¨Ø­Ø°ÙÙ‡Ø§ ÙÙˆØ±Ø§Ù‹. (Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·Ø±: {risk_score}%)"
    else:  # English
        if classification == "SAFE":
            if has_trusted:
                return f"Message appears official from a trusted source. No clear fraud indicators detected. (Risk score: {risk_score}%)"
            return f"Message appears generally safe. No clear risk indicators found. (Risk score: {risk_score}%)"
        elif classification == "LOW_RISK":
            return f"Message contains some signs requiring moderate caution. Verify the source. (Risk score: {risk_score}%)"
        elif classification == "SUSPICIOUS":
            return f"Message contains several suspicious indicators. Exercise extreme caution and don't click any links. (Risk score: {risk_score}%)"
        else:  # HIGH_RISK
            return f"Message contains strong fraud indicators. Do not interact and delete immediately. (Risk score: {risk_score}%)"


def calculate_enhanced_risk(
    ml_score: float,
    llm_analysis: Optional[LLMAnalysis],
    urls: List[str],
    url_predictions: List[URLPrediction]
) -> tuple:
    """
    Enhanced risk calculation with complete technical details
    Returns: (risk_score, classification, classification_ar, technical_details)
    """
    
    # Check if all URLs are trusted
    all_trusted = all(is_trusted_domain(url) for url in urls) if urls else False
    
    # Check for critical red flags
    has_critical_flags = False
    if llm_analysis and llm_analysis.red_flags:
        critical = ['shortener', 'password', 'sensitive', 'otp', 'pin']
        has_critical_flags = any(c in ' '.join(llm_analysis.red_flags).lower() 
                                for c in critical)
    
    # Classify URL types
    url_types = []
    for pred in url_predictions:
        url_type = classify_url_type(pred.probability, pred.url)
        url_types.append(url_type)
    
    # Remove duplicates
    url_types = list(set(url_types))
    
    # === TRUST OVERRIDE ===
    if all_trusted and urls and not has_critical_flags:
        risk_score = 15.0
        classification = "SAFE"
        classification_ar = "Ø¢Ù…Ù†Ø© - Ø±Ø³Ø§Ù„Ø© Ø±Ø³Ù…ÙŠØ©"
        
        red_flags_details = [
            "ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· - Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø©",
            "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ø­ÙƒÙˆÙ…ÙŠØ© Ø±Ø³Ù…ÙŠØ© (.gov.sa)",
            "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù†ØªØ­Ø§Ù„ Ù‡ÙˆÙŠØ©",
            "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø·Ù„Ø¨ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø§Ø³Ø©"
        ]
    else:
        # === CALCULATE COMBINED SCORE ===
        if llm_analysis:
            llm_risk = llm_analysis.confidence
            if not llm_analysis.is_phishing:
                llm_risk = min(llm_risk, 35.0)
            combined = (ml_score * 0.4) + (llm_risk * 0.6)
        else:
            combined = ml_score
        
        risk_score = round(combined, 1)
        
        # === CLASSIFICATION ===
        if risk_score <= 30:
            classification = "SAFE"
            classification_ar = "Ø¢Ù…Ù†Ø©"
        elif risk_score <= 55:
            classification = "LOW_RISK"
            classification_ar = "Ù…Ù†Ø®ÙØ¶Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©"
        elif risk_score <= 75:
            classification = "SUSPICIOUS"
            classification_ar = "Ù…Ø´Ø¨ÙˆÙ‡Ø©"
        else:
            classification = "HIGH_RISK"
            classification_ar = "Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø®Ø·ÙˆØ±Ø©"
        
        # === DETAILED RED FLAGS ===
        red_flags_details = []
        
        # Analyze each URL
        for pred in url_predictions:
            if pred.probability >= 0.7:
                red_flags_details.append(
                    f"ğŸš¨ Ø±Ø§Ø¨Ø· Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø®Ø·ÙˆØ±Ø©: {pred.url} (Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: {pred.probability*100:.0f}%)"
                )
            elif pred.probability >= 0.5:
                red_flags_details.append(
                    f"âš ï¸ Ø±Ø§Ø¨Ø· Ù…Ø´Ø¨ÙˆÙ‡: {pred.url} (Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: {pred.probability*100:.0f}%)"
                )
        
        # Add LLM red flags
        if llm_analysis and llm_analysis.red_flags_ar:
            for flag in llm_analysis.red_flags_ar:
                if flag != "Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ø­ØªÙŠØ§Ù„ ÙˆØ§Ø¶Ø­Ø©":
                    red_flags_details.append(f"ğŸ§  {flag}")
        
        # If no specific flags, add general assessment
        if not red_flags_details:
            if risk_score < 30:
                red_flags_details.append("âœ… Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø¤Ø´Ø±Ø§Øª Ø®Ø·Ø± ÙˆØ§Ø¶Ø­Ø©")
            else:
                red_flags_details.append("âš ï¸ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªØ³ØªØ¯Ø¹ÙŠ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡")
    
    # === TECHNICAL DETAILS ===
    analysis_method = "Hybrid (ML + LLM)" if llm_analysis else "ML Only"
    if all_trusted and urls:
        analysis_method += " + Trust Override"
    
    technical_details = TechnicalDetails(
        urls_found=len(urls),
        url_types=url_types if url_types else ["No URLs"],
        ml_risk_score=round(ml_score, 1),
        llm_confidence=round(llm_analysis.confidence, 1) if llm_analysis else None,
        trusted_source=all_trusted if urls else False,
        red_flags_details=red_flags_details,
        analysis_method=analysis_method,
        features_analyzed=41 * len(urls) if urls else 0
    )
    
    return risk_score, classification, classification_ar, technical_details


@app.get("/")
async def root():
    return {
        "status": "online",
        "service": "Tanabbah Enhanced API",
        "version": "2.2.0",
        "features": ["complete_technical_insights", "multi_language", "trust_override"],
        "model_loaded": MODEL_LOADED,
        "llm_enabled": is_llm_available()
    }


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "2.2.0",
        "model_loaded": MODEL_LOADED,
        "llm_enabled": is_llm_available()
    }


@app.post("/api/analyze", response_model=AnalyzeResponse)
async def analyze_message(request: AnalyzeRequest):
    """
    Enhanced analysis endpoint with complete technical details
    """
    try:
        logger.info(f"Analyzing message (length: {len(request.message)}, language: {request.language})")
        
        # Validation
        if not request.message or len(request.message.strip()) == 0:
            raise HTTPException(status_code=400, detail="Message cannot be empty")
        
        if len(request.message) > 10000:
            raise HTTPException(status_code=400, detail="Message too long")
        
        message = request.message.strip()
        enable_llm = request.enable_llm and is_llm_available()
        language = request.language or "ar"
        
        # Extract URLs
        urls = extract_urls(message)
        logger.info(f"Found {len(urls)} URLs: {urls}")
        
        # Analyze URLs with ML
        url_predictions = []
        total_prob = 0.0
        
        for url in urls:
            try:
                pred = predict_url(url)
                url_predictions.append(pred)
                total_prob += pred.probability
            except Exception as e:
                logger.error(f"Error predicting URL {url}: {e}")
                url_predictions.append(URLPrediction(
                    url=url,
                    prediction=0,
                    probability=0.5,
                    features={}
                ))
                total_prob += 0.5
        
        # ML risk score
        ml_risk_score = round(
            (total_prob / len(url_predictions) * 100) if url_predictions else 0.0,
            2
        )
        
        # LLM analysis
        llm_analysis = None
        if enable_llm:
            try:
                llm_analysis = await analyze_message_with_llm(message, urls)
            except Exception as e:
                logger.error(f"LLM analysis failed: {e}")
        
        # Calculate enhanced risk with technical details
        risk_score, classification, classification_ar, technical_details = \
            calculate_enhanced_risk(ml_risk_score, llm_analysis, urls, url_predictions)
        
        # Get explanations
        has_trusted = technical_details.trusted_source
        explanation_ar = get_explanation(classification, risk_score, has_trusted, "ar")
        explanation_en = get_explanation(classification, risk_score, has_trusted, "en")
        
        # Get action guidance
        action_ar = get_action_guidance(classification, "ar")
        action_en = get_action_guidance(classification, "en")
        
        # Get red flags
        red_flags_ar = llm_analysis.red_flags_ar if llm_analysis else ["Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ø­ØªÙŠØ§Ù„ ÙˆØ§Ø¶Ø­Ø©"]
        red_flags_en = llm_analysis.red_flags if llm_analysis else ["No significant red flags detected"]
        
        # Filter out "no red flags" messages if risk is high
        if risk_score > 30:
            red_flags_ar = [f for f in red_flags_ar if "Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù" not in f]
            red_flags_en = [f for f in red_flags_en if "no" not in f.lower() and "significant" not in f.lower()]
        
        logger.info(f"Analysis complete: {classification} ({risk_score}%)")
        
        return AnalyzeResponse(
            message=message,
            classification=classification,
            classification_ar=classification_ar,
            explanation=explanation_en if language == "en" else explanation_ar,
            explanation_ar=explanation_ar,
            red_flags=red_flags_en,
            red_flags_ar=red_flags_ar,
            risk_score=risk_score,
            action=action_en if language == "en" else action_ar,
            action_ar=action_ar,
            technical_details=technical_details,
            urls_found=len(urls),
            url_predictions=url_predictions,
            ml_risk_score=ml_risk_score,
            llm_analysis=llm_analysis,
            combined_risk_score=risk_score,
            status="success"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Analysis error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


@app.post("/api/report")
async def report_message(request: dict):
    """Report phishing message"""
    try:
        message = request.get("message", "")
        
        if not message or len(message.strip()) == 0:
            raise HTTPException(status_code=400, detail="Message cannot be empty")
        
        logger.info(f"Report received: {len(message)} chars")
        
        return {
            "status": "success",
            "message": "Report received successfully",
            "reference_id": f"TN-{hash(message) % 100000:05d}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Report error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to submit report")


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")